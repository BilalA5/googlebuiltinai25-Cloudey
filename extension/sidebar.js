import { icons, getIconHTML } from './icons.js';

console.log('Cloudey side panel loaded');

// Audio recording variables
let microphonePermission = false;
let isRecording = false;
let mediaRecorder = null;
let audioChunks = [];
let isUserInitiated = false; // Flag to ensure user-initiated microphone access
let micButtonCooldown = false; // Prevent rapid clicking
let listeningStartTime = null; // Track recording start time
let listeningTimer = null; // Timer for updating display
let speechRecognition = null; // Web Speech API for real-time transcription

// DOM elements
const chatInput = document.getElementById('chat-input');
const sendBtn = document.getElementById('send-btn');
const pauseBtn = document.getElementById('pause-btn');
const stopBtn = document.getElementById('stop-btn');
const micBtn = document.getElementById('mic-btn');
const attachBtn = document.getElementById('attach-btn') || document.querySelector('.prompt-action-btn[title="Attach image"]');
const fileInput = document.getElementById('file-input');
const messagesContainer = document.getElementById('messages-container');
const attachmentChips = document.getElementById('attachment-chips');
const assistantTypingRow = document.getElementById('assistant-typing-row');
const fabToggle = document.getElementById('fab-toggle');
const fabActions = document.getElementById('fab-actions');
const closeBtn = document.getElementById('close-btn');
const ariaPolite = document.getElementById('aria-polite');
const ariaAssertive = document.getElementById('aria-assertive');


// State
let isListening = false;
let isStreaming = false;
let conversationHistory = [];
let attachedFiles = [];
let typewriterAbortController = null;
let includeContext = true; // Default to including context
let isSending = false; // Prevent double sends
const MAX_FILES = 5;
const MAX_FILE_SIZE = 25 * 1024 * 1024; // 25MB

// Gemini API integration handled via background script

// Initialize icons
function initializeIcons() {
  // Note: Icons are now embedded directly in HTML as SVG, so no JS initialization needed
  console.log('Icons embedded in HTML structure');
}

// Initialize Speech Recognition
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
let recognition = null;

if (SpeechRecognition) {
  recognition = new SpeechRecognition();
  recognition.continuous = false;
  recognition.interimResults = false;
  recognition.lang = 'en-US';
  
  recognition.onstart = () => {
    isListening = true;
    if (micBtn) micBtn.classList.add('listening');
    announceToScreenReader('Listening for voice input', 'polite');
  };
  
  recognition.onend = () => {
    isListening = false;
    if (micBtn) micBtn.classList.remove('listening');
  };
  
  recognition.onresult = (event) => {
    let transcript = '';
    for (let i = event.resultIndex; i < event.results.length; i++) {
      transcript += event.results[i][0].transcript;
    }
    if (transcript) {
      chatInput.value = transcript;
      autoResizeTextarea();
      chatInput.focus();
      announceToScreenReader('Voice input received. Review and press Enter to send.', 'polite');
    }
  };
  
  recognition.onerror = (event) => {
    console.error('Speech recognition error:', event.error);
    
    let errorMessage = 'Voice input failed. Please try again.';
    
    if (event.error === 'not-allowed') {
      errorMessage = 'Microphone permission denied. Please enable microphone access in Chrome settings.';
      if (micBtn) {
        micBtn.disabled = true;
        micBtn.title = 'Microphone access denied';
      }
    } else if (event.error === 'no-speech') {
      errorMessage = 'No speech detected. Please try again.';
    } else if (event.error === 'audio-capture') {
      errorMessage = 'No microphone found. Please check your microphone.';
    } else if (event.error === 'network') {
      errorMessage = 'Network error. Please check your connection.';
    }
    
    announceToScreenReader(errorMessage, 'assertive');
    isListening = false;
    if (micBtn) micBtn.classList.remove('listening');
  };
}

// Auto-resize textarea (0-4 lines)
function autoResizeTextarea() {
  chatInput.style.height = 'auto';
  const newHeight = Math.min(chatInput.scrollHeight, 160);
  chatInput.style.height = newHeight + 'px';
}

// Keyboard shortcuts
document.addEventListener('keydown', (e) => {
  // Enter to send (without Shift)
  if (e.key === 'Enter' && !e.shiftKey && document.activeElement === chatInput) {
    e.preventDefault();
    sendMessage();
  }
  
  // Cmd/Ctrl + Enter to send
  if (e.key === 'Enter' && (e.metaKey || e.ctrlKey) && document.activeElement === chatInput) {
    e.preventDefault();
    sendMessage();
  }
  
  // Escape to cancel recording or blur
  if (e.key === 'Escape') {
    if (isListening && recognition) {
      recognition.stop();
    } else if (document.activeElement === chatInput) {
      chatInput.blur();
    }
  }
  
  // / to focus chat bar
  if (e.key === '/' && document.activeElement !== chatInput) {
    e.preventDefault();
    chatInput.focus();
  }
  
  // S to skip typewriter
  if (e.key === 's' || e.key === 'S') {
    if (isStreaming) {
      skipTypewriter();
    }
  }
});

// Audio recording functions
async function toggleRecording() {
  // Prevent rapid clicking
  if (micButtonCooldown) {
    console.log('ğŸ¤ Microphone button in cooldown, ignoring click');
    return;
  }
  
  if (isRecording) {
    stopRecording();
  } else {
    // Set user-initiated flag before requesting microphone access
    isUserInitiated = true;
    
    // Set cooldown to prevent rapid clicking
    micButtonCooldown = true;
    setTimeout(() => {
      micButtonCooldown = false;
    }, 2000); // 2 second cooldown
    
    await startRecording();
  }
}

async function requestMicrophonePermission() {
  try {
    console.log('ğŸ¤ Requesting microphone permission...');
    
    // Check if we're in a user-initiated context
    if (!isUserInitiated) {
      throw new Error('Microphone access must be initiated by user interaction');
    }
    
    // Get the active tab first
    const tabs = await chrome.tabs.query({ active: true, currentWindow: true });
    if (tabs.length === 0) {
      throw new Error('No active tab found');
    }
    
    // Try content script approach first
    try {
      const response = await new Promise((resolve, reject) => {
        const timeout = setTimeout(() => {
          reject(new Error('Content script timeout - no response received'));
        }, 5000); // 5 second timeout
        
        chrome.tabs.sendMessage(tabs[0].id, { 
          action: 'requestMicrophonePermission' 
        }, (response) => {
          clearTimeout(timeout);
          if (chrome.runtime.lastError) {
            reject(new Error(chrome.runtime.lastError.message));
          } else {
            resolve(response);
          }
        });
      });
      
      if (response && response.success) {
        microphonePermission = true;
        console.log('âœ… Microphone permission granted via content script');
        
        // Show success message
        addMessage('system', 'ğŸ¤ **Microphone Access Granted!**\n\nYou can now use voice input. Click the microphone button to start recording.');
        
        return true;
      } else {
        throw new Error(response?.error || 'Permission denied');
      }
      
    } catch (contentScriptError) {
      console.log('âš ï¸ Content script approach failed, trying direct approach:', contentScriptError.message);
      
      // Fallback: Try direct microphone access in sidebar
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            sampleRate: 44100
          } 
        });
        
        // Stop the stream immediately - we just needed permission
        stream.getTracks().forEach(track => track.stop());
        
        microphonePermission = true;
        console.log('âœ… Microphone permission granted via direct access');
        
        // Show success message
        addMessage('system', 'ğŸ¤ **Microphone Access Granted!**\n\nYou can now use voice input. Click the microphone button to start recording.');
        
        return true;
        
      } catch (directError) {
        console.log('âŒ Direct microphone access also failed:', directError.message);
        throw directError; // Re-throw the direct error
      }
    }
    
  } catch (error) {
    console.log('âŒ Microphone permission denied:', error);
    handleMicrophonePermissionError(error);
    return false;
  }
}

async function startRecording() {
  console.log('ğŸ¤ Starting recording process...');
  
  if (!microphonePermission) {
    console.log('ğŸ¤ No permission yet, requesting...');
    const hasPermission = await requestMicrophonePermission();
    if (!hasPermission) {
      console.log('âŒ Permission denied, cannot start recording');
      // Reset UI state when permission is denied
      resetMicrophoneUI();
      return;
    }
  }
  
  try {
    console.log('ğŸ¤ Starting recording via content script...');
    
    // Get the active tab first
    const tabs = await chrome.tabs.query({ active: true, currentWindow: true });
    if (tabs.length === 0) {
      throw new Error('No active tab found');
    }
    
    // Try content script approach first
    try {
      const response = await new Promise((resolve, reject) => {
        const timeout = setTimeout(() => {
          reject(new Error('Content script timeout - no response received'));
        }, 5000); // 5 second timeout
        
        chrome.tabs.sendMessage(tabs[0].id, { 
          action: 'startRecording' 
        }, (response) => {
          clearTimeout(timeout);
          if (chrome.runtime.lastError) {
            reject(new Error(chrome.runtime.lastError.message));
          } else {
            resolve(response);
          }
        });
      });
      
      if (response && response.success) {
        isRecording = true;
        
        // Update UI with visual feedback
        micBtn.classList.add('recording');
        micBtn.classList.add('active'); // Add blue glow effect
        micBtn.title = 'Stop recording';
        
        // Show audio visualizer
        showAudioVisualizer();
        
        // Start minimal listening UI
        startMinimalListeningUI();
        
        console.log('ğŸ¤ Recording started successfully via content script');
        return;
      } else {
        throw new Error(response?.error || 'Recording failed');
      }
      
    } catch (contentScriptError) {
      console.log('âš ï¸ Content script recording failed, trying direct approach:', contentScriptError.message);
      
      // Fallback: Try direct recording in sidebar
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100
        } 
      });
      
      mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'audio/webm;codecs=opus'
      });
      
      audioChunks = [];
      
      mediaRecorder.ondataavailable = (event) => {
        audioChunks.push(event.data);
      };
      
      mediaRecorder.onstop = async () => {
        console.log('ğŸ¤ Recording stopped, processing audio...');
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        await processAudio(audioBlob);
        
        // Clean up
        stream.getTracks().forEach(track => track.stop());
      };
      
      mediaRecorder.start();
      isRecording = true;
      
      // Update UI with visual feedback
      micBtn.classList.add('recording');
      micBtn.classList.add('active'); // Add blue glow effect
      micBtn.title = 'Stop recording';
      
      // Show audio visualizer
      showAudioVisualizer();
      
      // Start minimal listening UI
      startMinimalListeningUI();
      
      console.log('ğŸ¤ Recording started successfully via direct access');
    }
    
  } catch (error) {
    console.log('âŒ Recording failed:', error);
    
    // Reset UI state on any error
    resetMicrophoneUI();
    
    // Handle the error with proper messaging
    handleMicrophonePermissionError(error);
  }
}

function stopRecording() {
  if (isRecording) {
    console.log('ğŸ¤ Stopping recording...');
    
    // Try content script approach first
    chrome.tabs.query({ active: true, currentWindow: true }, (tabs) => {
      if (tabs.length > 0) {
        chrome.tabs.sendMessage(tabs[0].id, { 
          action: 'stopRecording' 
        }, (response) => {
          if (chrome.runtime.lastError) {
            console.log('âš ï¸ Content script stop failed, using direct approach');
            // Fallback: Stop direct recording
            if (mediaRecorder && mediaRecorder.state === 'recording') {
              mediaRecorder.stop();
            }
          }
        });
      } else {
        // Fallback: Stop direct recording
        if (mediaRecorder && mediaRecorder.state === 'recording') {
          mediaRecorder.stop();
        }
      }
    });
    
    isRecording = false;
    
    // Update UI
    micBtn.classList.remove('recording');
    micBtn.classList.remove('active'); // Remove blue glow effect
    micBtn.classList.add('processing');
    micBtn.title = 'Processing...';
    
    // Hide audio visualizer
    hideAudioVisualizer();
    
    // Stop minimal listening UI
    stopMinimalListeningUI();
    
    console.log('ğŸ¤ Recording stopped');
  }
}

async function processAudio(audioBlob) {
  try {
    // Convert audio to base64
    const base64Audio = await blobToBase64(audioBlob);
    
    // Call Google Speech-to-Text API
    const transcription = await callGoogleSpeechAPI(base64Audio);
    
    if (transcription && transcription.trim()) {
      // Insert transcribed text into chat input
      chatInput.value = transcription;
      chatInput.focus();
      
      // Show success message
      addMessage('system', `ğŸ¤ **Voice Input Received**\n\n"${transcription}"`);
      
      console.log('âœ… Transcription successful:', transcription);
    } else {
      addMessage('system', 'âŒ **No Speech Detected**\n\nPlease try speaking more clearly or check your microphone.');
    }
    
  } catch (error) {
    console.log('âŒ Transcription failed:', error);
    addMessage('system', 'âŒ **Transcription Failed**\n\nUnable to process audio. Please try again.');
  } finally {
    // Reset UI
    micBtn.classList.remove('processing');
    micBtn.title = 'Voice input';
  }
}

async function callGoogleSpeechAPI(base64Audio) {
  // Get API key from background script
  const response = await chrome.runtime.sendMessage({ action: 'getApiKey' });
  const apiKey = response?.apiKey;
  
  if (!apiKey) {
    throw new Error('API key not available');
  }
  
  const speechResponse = await fetch('https://speech.googleapis.com/v1/speech:recognize', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${apiKey}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      config: {
        encoding: 'WEBM_OPUS',
        sampleRateHertz: 44100,
        languageCode: 'en-US',
        alternativeLanguageCodes: ['es', 'fr', 'de', 'it', 'pt'],
        enableAutomaticPunctuation: true,
        model: 'latest_long'
      },
      audio: {
        content: base64Audio.split(',')[1] // Remove data:audio/webm;base64, prefix
      }
    })
  });
  
  const result = await speechResponse.json();
  
  if (result.results && result.results.length > 0) {
    return result.results[0].alternatives[0].transcript;
  }
  
  return null;
}

// Minimal Listening Animation Controller
function startMinimalListeningUI() {
  const listeningOverlay = document.getElementById('listening-overlay');
  const listeningTimerElement = document.getElementById('listening-timer');
  const listeningStopBtn = document.getElementById('listening-stop-btn');
  
  if (listeningOverlay) {
    listeningOverlay.classList.add('active');
    console.log('ğŸ¤ Started minimal listening UI');
  }
  
  // Add stop button event listener
  if (listeningStopBtn) {
    listeningStopBtn.addEventListener('click', () => {
      console.log('ğŸ›‘ Stop button clicked');
      handleStopButtonClick();
    });
  }
  
  // Start timer
  listeningStartTime = Date.now();
  listeningTimer = setInterval(updateListeningTimer, 100);
  
  // Start real-time speech recognition
  startSpeechRecognition();
}

function stopMinimalListeningUI() {
  const listeningOverlay = document.getElementById('listening-overlay');
  
  if (listeningOverlay) {
    listeningOverlay.classList.remove('active');
    console.log('ğŸ¤ Stopped minimal listening UI');
  }
  
  // Stop timer
  if (listeningTimer) {
    clearInterval(listeningTimer);
    listeningTimer = null;
  }
  
  // Stop speech recognition
  stopSpeechRecognition();
}

function updateListeningTimer() {
  const listeningTimerElement = document.getElementById('listening-timer');
  if (listeningTimerElement && listeningStartTime) {
    const elapsed = Math.floor((Date.now() - listeningStartTime) / 1000);
    const minutes = Math.floor(elapsed / 60);
    const seconds = elapsed % 60;
    listeningTimerElement.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
  }
}

function startSpeechRecognition() {
  // Check if Speech Recognition is supported
  if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
    console.log('âŒ Speech Recognition not supported');
    addMessage('system', 'ğŸ¤ **Speech Recognition Not Supported**\n\nYour browser does not support speech recognition. Please use text input instead.');
    return;
  }
  
  const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  speechRecognition = new SpeechRecognition();
  
  // Configure speech recognition for better accuracy
  speechRecognition.continuous = true;
  speechRecognition.interimResults = true;
  speechRecognition.lang = 'en-US';
  speechRecognition.maxAlternatives = 1;
  
  let finalTranscript = '';
  let interimTranscript = '';
  
  speechRecognition.onstart = () => {
    console.log('ğŸ¤ Speech recognition started');
    finalTranscript = '';
    interimTranscript = '';
  };
  
  speechRecognition.onresult = (event) => {
    interimTranscript = '';
    
    for (let i = event.resultIndex; i < event.results.length; i++) {
      const transcript = event.results[i][0].transcript;
      const confidence = event.results[i][0].confidence;
      
      if (event.results[i].isFinal) {
        finalTranscript += transcript + ' ';
        console.log('ğŸ¤ Final transcript:', transcript, 'Confidence:', confidence);
      } else {
        interimTranscript += transcript;
        console.log('ğŸ¤ Interim transcript:', transcript);
      }
    }
    
    // Update the chat input with real-time transcription
    if (chatInput) {
      const displayText = finalTranscript + interimTranscript;
      chatInput.value = displayText;
      chatInput.focus();
      
      // Auto-resize textarea
      chatInput.style.height = 'auto';
      chatInput.style.height = Math.min(chatInput.scrollHeight, 120) + 'px';
    }
  };
  
  speechRecognition.onerror = (event) => {
    console.log('âŒ Speech recognition error:', event.error);
    
    // Don't show error messages for aborted recognition (when user stops)
    if (event.error === 'aborted') {
      console.log('ğŸ¤ Speech recognition aborted by user');
      return;
    }
    
    let errorMessage = '';
    switch (event.error) {
      case 'no-speech':
        errorMessage = 'ğŸ¤ **No Speech Detected**\n\nPlease speak clearly into your microphone.';
        break;
      case 'audio-capture':
        errorMessage = 'ğŸ¤ **Microphone Error**\n\nCould not access your microphone. Please check your microphone settings.';
        break;
      case 'not-allowed':
        errorMessage = 'ğŸ¤ **Permission Denied**\n\nMicrophone access was denied. Please allow microphone access and try again.';
        break;
      case 'network':
        errorMessage = 'ğŸ¤ **Network Error**\n\nSpeech recognition requires an internet connection. Please check your connection.';
        break;
      default:
        errorMessage = `ğŸ¤ **Speech Recognition Error**\n\nError: ${event.error}\n\nPlease try again.`;
    }
    
    addMessage('system', errorMessage);
  };
  
  speechRecognition.onend = () => {
    console.log('ğŸ¤ Speech recognition ended');
    
    // Only process transcript if we have one and it wasn't aborted
    if (finalTranscript.trim()) {
      console.log('ğŸ¤ Processing final transcript:', finalTranscript.trim());
      // Don't process here - let the stop button handle it
    } else if (interimTranscript.trim()) {
      console.log('ğŸ¤ Processing interim transcript:', interimTranscript.trim());
      // Don't process here - let the stop button handle it
    } else {
      console.log('ğŸ¤ No speech detected');
    }
  };
  
  try {
    speechRecognition.start();
    console.log('ğŸ¤ Speech recognition started successfully');
  } catch (error) {
    console.log('âŒ Failed to start speech recognition:', error);
    addMessage('system', 'ğŸ¤ **Speech Recognition Failed**\n\nCould not start speech recognition. Please try again.');
  }
}

function stopSpeechRecognition() {
  if (speechRecognition) {
    speechRecognition.stop();
    speechRecognition = null;
  }
}

function processSpeechTranscript(transcript) {
  console.log('ğŸ¤ Processing speech transcript:', transcript);
  
  // Add transcript to message input
  if (chatInput && transcript) {
    chatInput.value = transcript;
    chatInput.focus();
    
    // Show success message
    addMessage('system', `ğŸ¤ **Voice Input Received**\n\n"${transcript}"\n\nClick send to process your voice message.`);
  }
}

function handleStopButtonClick() {
  const listeningStopBtn = document.getElementById('listening-stop-btn');
  
  // Show spinner animation
  if (listeningStopBtn) {
    listeningStopBtn.classList.add('processing');
    listeningStopBtn.textContent = '';
  }
  
  // Stop speech recognition first
  stopSpeechRecognition();
  
  // Get the current transcript from chat input
  const currentTranscript = chatInput ? chatInput.value.trim() : '';
  
  // Stop the recording
  stopRecording();
  
  // Process the transcript after a short delay
  setTimeout(() => {
    if (currentTranscript) {
      processSpeechTranscript(currentTranscript);
    } else {
      addMessage('system', 'ğŸ¤ **No Speech Detected**\n\nPlease try speaking again.');
    }
    
    // Reset stop button
    if (listeningStopBtn) {
      listeningStopBtn.classList.remove('processing');
      listeningStopBtn.textContent = 'â¹';
    }
  }, 1000);
}

function handleMicrophonePermissionError(error) {
  let errorMessage = '';
  let recoverySteps = '';
  
  if (error.message.includes('NotAllowedError') || error.message.includes('Permission dismissed')) {
    errorMessage = 'ğŸ¤ **Microphone Permission Denied**';
    recoverySteps = `
**The browser blocked microphone access. Here's how to fix it:**

1. **Click the microphone button again** and select "Allow" when prompted
2. **Check Chrome settings:**
   - Go to chrome://settings/content/microphone
   - Find this website/extension in the list
   - Set it to "Allow"
3. **Extension permissions:**
   - Right-click the Cloudey extension icon
   - Select "Manage extension"
   - Check if microphone access is enabled
4. **Refresh and retry:**
   - Reload this page
   - Try the microphone button again

**Important:** Make sure to click "Allow" instead of dismissing the permission dialog.`;
  } else if (error.message.includes('NotFoundError')) {
    errorMessage = 'ğŸ¤ **No Microphone Found**';
    recoverySteps = `
**No microphone device detected:**

1. **Connect a microphone** to your computer
2. **Check device settings:**
   - Go to chrome://settings/content/microphone
   - Verify a microphone is selected
3. **Test your microphone** in other applications
4. **Try refreshing** this page and try again`;
  } else if (error.message.includes('NotSupportedError')) {
    errorMessage = 'ğŸ¤ **Microphone Not Supported**';
    recoverySteps = `
**Microphone access is not supported:**

1. **Update Chrome** to the latest version
2. **Use HTTPS** - microphone requires secure connection
3. **Try a different browser** if the issue persists
4. **Check system permissions** for microphone access`;
  } else if (error.message.includes('Content script timeout') || error.message.includes('message port closed')) {
    errorMessage = 'ğŸ¤ **Extension Communication Failed**';
    recoverySteps = `
**Extension communication error:**

1. **Refresh the current page** and try again
2. **Reload the extension:**
   - Go to chrome://extensions/
   - Find Cloudey and click the refresh icon
3. **Check if extension is enabled** in chrome://extensions/
4. **Try clicking the microphone button** again`;
  } else if (error.message.includes('user-initiated')) {
    errorMessage = 'ğŸ¤ **User Interaction Required**';
    recoverySteps = `
**Microphone access must be initiated by user interaction:**

1. **Click the microphone button** directly
2. **Don't use keyboard shortcuts** for microphone access
3. **Ensure you're clicking** the button, not using automated triggers
4. **Try clicking again** if the first attempt failed`;
  } else {
    errorMessage = 'ğŸ¤ **Microphone Access Error**';
    recoverySteps = `
**Unexpected error occurred:**

**Error:** ${error.message}

**Troubleshooting steps:**
1. **Refresh this page** and try again
2. **Check Chrome settings:** chrome://settings/content/microphone
3. **Verify extension permissions** in chrome://extensions/
4. **Try a different microphone** if available
5. **Contact support** if the issue persists`;
  }
  
  // Show error message with recovery steps
  addMessage('system', `${errorMessage}\n\n${recoverySteps}`);
  
  // Reset microphone button state
  if (micBtn) {
    micBtn.classList.remove('recording', 'processing', 'active');
    micBtn.title = 'Voice input';
  }
  
  // Stop minimal listening UI
  stopMinimalListeningUI();
  
  // Show microphone status indicator only for certain errors (not permission dismissed)
  if (!error.message.includes('Permission dismissed')) {
    showMicrophoneStatusIndicator();
  }
}

function showMicrophoneStatusIndicator() {
  const micStatusIndicator = document.getElementById('mic-status-indicator');
  const micRetryBtn = document.getElementById('mic-retry-btn');
  
  if (micStatusIndicator) {
    micStatusIndicator.classList.remove('hidden');
    
    // Add retry button event listener
    if (micRetryBtn) {
      micRetryBtn.addEventListener('click', () => {
        hideMicrophoneStatusIndicator();
        // Reset user-initiated flag and try again
        isUserInitiated = true;
        startRecording();
      });
    }
  }
}

function hideMicrophoneStatusIndicator() {
  const micStatusIndicator = document.getElementById('mic-status-indicator');
  if (micStatusIndicator) {
    micStatusIndicator.classList.add('hidden');
  }
}

function resetMicrophoneUI() {
  console.log('ğŸ”„ Resetting microphone UI state...');
  
  // Reset microphone button state
  if (micBtn) {
    micBtn.classList.remove('recording', 'processing', 'active');
    micBtn.title = 'Voice input';
  }
  
  // Reset recording state
  isRecording = false;
  microphonePermission = false;
  micButtonCooldown = false; // Reset cooldown
  
  // Hide audio visualizer
  hideAudioVisualizer();
  
  // Stop minimal listening UI
  stopMinimalListeningUI();
  
  // Hide microphone status indicator
  hideMicrophoneStatusIndicator();
  
  console.log('âœ… Microphone UI state reset complete');
}

function getFileIcon(fileName) {
  const extension = fileName.split('.').pop().toLowerCase();
  
  const iconMap = {
    // Images
    'jpg': 'ğŸ–¼ï¸', 'jpeg': 'ğŸ–¼ï¸', 'png': 'ğŸ–¼ï¸', 'gif': 'ğŸ–¼ï¸', 'bmp': 'ğŸ–¼ï¸', 'webp': 'ğŸ–¼ï¸', 'svg': 'ğŸ–¼ï¸',
    // Documents
    'pdf': 'ğŸ“„', 'doc': 'ğŸ“„', 'docx': 'ğŸ“„', 'txt': 'ğŸ“„', 'rtf': 'ğŸ“„',
    // Spreadsheets
    'xls': 'ğŸ“Š', 'xlsx': 'ğŸ“Š', 'csv': 'ğŸ“Š',
    // Presentations
    'ppt': 'ğŸ“½ï¸', 'pptx': 'ğŸ“½ï¸',
    // Code
    'js': 'ğŸ“', 'ts': 'ğŸ“', 'html': 'ğŸ“', 'css': 'ğŸ“', 'json': 'ğŸ“', 'xml': 'ğŸ“', 'md': 'ğŸ“',
    // Archives
    'zip': 'ğŸ“¦', 'rar': 'ğŸ“¦', '7z': 'ğŸ“¦', 'tar': 'ğŸ“¦', 'gz': 'ğŸ“¦',
    // Audio
    'mp3': 'ğŸµ', 'wav': 'ğŸµ', 'flac': 'ğŸµ', 'aac': 'ğŸµ', 'ogg': 'ğŸµ',
    // Video
    'mp4': 'ğŸ¬', 'avi': 'ğŸ¬', 'mov': 'ğŸ¬', 'wmv': 'ğŸ¬', 'flv': 'ğŸ¬', 'webm': 'ğŸ¬',
    // Default
    'default': 'ğŸ“'
  };
  
  return iconMap[extension] || iconMap['default'];
}

function handleAudioTranscriptionResult(transcript) {
  console.log('ğŸ¤ Audio transcription result:', transcript);
  
  // Update UI
  micBtn.classList.remove('processing');
  micBtn.title = 'Voice input';
  
  // Reset listening animation
  resetListeningAnimation();
  
  // Add transcript to message input
  const messageInput = document.getElementById('message-input');
  if (messageInput && transcript) {
    messageInput.value = transcript;
    messageInput.focus();
    
    // Show success message
    addMessage('system', `ğŸ¤ **Voice Input Received**\n\n"${transcript}"\n\nClick send to process your voice message.`);
  }
}

function showAudioVisualizer() {
  const visualizer = document.createElement('div');
  visualizer.className = 'audio-visualizer';
  visualizer.innerHTML = `
    <div class="audio-bar"></div>
    <div class="audio-bar"></div>
    <div class="audio-bar"></div>
    <div class="audio-bar"></div>
  `;
  
  micBtn.appendChild(visualizer);
}

function hideAudioVisualizer() {
  const visualizer = document.querySelector('.audio-visualizer');
  if (visualizer) {
    visualizer.remove();
  }
}

// Event listeners
sendBtn.addEventListener('click', sendMessage);
pauseBtn.addEventListener('click', stopGeneration);
chatInput.addEventListener('input', () => {
  autoResizeTextarea();
  updateSendButtonState();
});
chatInput.addEventListener('keydown', (e) => {
  if (e.key === 'Enter' && !e.shiftKey) {
    e.preventDefault();
    sendMessage();
  }
});

if (micBtn) {
  micBtn.addEventListener('click', async () => {
  if (!recognition) {
    announceToScreenReader('Speech recognition not supported in this browser', 'assertive');
    return;
  }
    
  if (isListening) {
    recognition.stop();
  } else {
      // Check if microphone permission is granted before starting
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        // Permission granted, stop the test stream and start recognition
        stream.getTracks().forEach(track => track.stop());
    recognition.start();
      } catch (error) {
        const errorName = error.name || 'Unknown error';
        const errorMessage = error.message || '';
        console.error('Microphone permission denied:', errorName, errorMessage);
        
        let userMessage = 'Microphone permission denied. Please enable microphone access in Chrome settings.';
        if (errorName === 'NotAllowedError' || errorName === 'PermissionDeniedError') {
          userMessage = 'Microphone access denied. Please allow microphone access in Chrome settings and reload.';
        } else if (errorName === 'NotFoundError') {
          userMessage = 'No microphone found. Please connect a microphone.';
        } else if (errorName === 'NotReadableError') {
          userMessage = 'Microphone is being used by another application. Please close other apps using the microphone.';
        }
        
        announceToScreenReader(userMessage, 'assertive');
        micBtn.disabled = true;
        micBtn.title = 'Microphone access denied';
      }
    }
  });
}

// File input is now handled by the action button
if (fileInput) {
fileInput.addEventListener('change', handleFileSelection);
}

if (stopBtn) {
stopBtn.addEventListener('click', stopGeneration);
}

// Close button
if (closeBtn) {
closeBtn.addEventListener('click', () => {
  window.close();
});
}


// Action button toggle (handles both file attachments and quick actions)
if (fabToggle) {
fabToggle.addEventListener('click', () => {
  const isOpen = !fabActions.classList.contains('hidden');
  if (isOpen) {
    fabActions.classList.add('hidden');
    fabToggle.classList.remove('active');
  } else {
    fabActions.classList.remove('hidden');
    fabToggle.classList.add('active');
  }
});
}

// Handle file input click when attachment button is clicked
if (attachBtn) {
  attachBtn.addEventListener('click', () => {
    if (fileInput) {
  fileInput.click();
    }
  });
}

// Microphone button event listener
if (micBtn) {
  micBtn.addEventListener('click', toggleRecording);
  console.log('ğŸ¤ Microphone button event listener added');
} else {
  console.log('âŒ Microphone button not found');
}

// FAB actions
document.querySelectorAll('.fab-action').forEach(btn => {
  btn.addEventListener('click', async (e) => {
    const action = btn.dataset.action;
    fabActions.classList.add('hidden');
    fabToggle.classList.remove('active');
    await handleFabAction(action);
  });
});


// Toggle buttons
const agentToggle = document.getElementById('agent-toggle');

// Agent Mode elements
const agentDialog = document.getElementById('agent-permission-dialog');
const agentDialogClose = document.getElementById('agent-dialog-close');
const agentDialogCancel = document.getElementById('agent-dialog-cancel');
const agentDialogConfirm = document.getElementById('agent-dialog-confirm');

const agentActionsList = document.getElementById('agent-actions-list');

// Agent Mode state
let isAgentMode = false;

// Toggle system - only one can be active at a time
function deactivateAllToggles() {
  agentToggle?.classList.remove('active');
  // Context is always enabled - no need to toggle
}

function activateToggle(toggle, mode) {
  deactivateAllToggles();
  toggle.classList.add('active');
  // Context is always enabled - no mode needed
}

// Translate main button - Direct translation on click
const translateMainBtn = document.getElementById('translate-main-btn');
if (translateMainBtn) {
  translateMainBtn.addEventListener('click', async () => {
    console.log('ğŸŒ Translate main button clicked');
    try {
      const tabs = await chrome.tabs.query({ active: true, currentWindow: true });
      const tab = tabs[0];
      
      if (tab) {
        const textToTranslate = await getSelectedText(tab.id);
        console.log('Selected text for translation:', textToTranslate);
        
        if (textToTranslate && textToTranslate.trim().length > 0) {
          addMessage('user', `Translate this: "${textToTranslate}"`);
          showTypingIndicator();
          
          console.log('Sending translation request to background script...');
          chrome.runtime.sendMessage(
            { action: 'translate', text: textToTranslate, from: 'auto', to: 'en' },
            (response) => {
              console.log('Translation response received:', response);
              handleFabResponse(response);
            }
          );
        } else {
          console.log('No text selected for translation');
          announceToScreenReader('Please select some text first', 'assertive');
        }
      }
    } catch (error) {
      console.error('Translate main button error:', error);
      addMessage('assistant', 'An error occurred while translating. Please try again.');
    }
  });
}

// Agent toggle
if (agentToggle) {
  agentToggle.addEventListener('click', () => {
    const isActive = agentToggle.classList.contains('active');
    
    if (isActive) {
      // Deactivate agent mode
      deactivateAgentMode();
    } else {
      // Show permission dialog for agent mode
      showAgentPermissionDialog();
    }
  });
}

// Agent permission dialog handlers
if (agentDialogClose) {
  agentDialogClose.addEventListener('click', () => {
    hideAgentPermissionDialog();
  });
}

if (agentDialogCancel) {
  agentDialogCancel.addEventListener('click', () => {
    hideAgentPermissionDialog();
  });
}

if (agentDialogConfirm) {
  agentDialogConfirm.addEventListener('click', () => {
    activateAgentMode();
    hideAgentPermissionDialog();
  });
}



// Agent Mode Functions
function showAgentPermissionDialog() {
  if (agentDialog) {
    // Generate sample actions for preview
    const sampleActions = [
      'Analyze page content',
      'Scroll to relevant sections',
      'Extract key information',
      'Fill form fields if needed'
    ];
    
    if (agentActionsList) {
      agentActionsList.innerHTML = sampleActions.map(action => 
        `<li>${action}</li>`
      ).join('');
    }
    
    agentDialog.classList.remove('hidden');
  }
}

function hideAgentPermissionDialog() {
  if (agentDialog) {
    agentDialog.classList.add('hidden');
  }
}

function activateAgentMode() {
  isAgentMode = true;
  agentToggle.classList.add('active');
  
  // Notify background script that agent mode is active
  chrome.runtime.sendMessage({ action: 'agentModeChanged', active: true });
  
  // Start border animation
  chrome.tabs.query({ active: true, currentWindow: true }, (tabs) => {
    chrome.tabs.sendMessage(tabs[0].id, { action: 'agentStart' });
  });
  
  announceToScreenReader('Agent mode enabled - Cloudey can take control of your screen', 'polite');
}

function deactivateAgentMode() {
  isAgentMode = false;
  agentToggle.classList.remove('active');
  
  // Notify background script that agent mode is inactive
  chrome.runtime.sendMessage({ action: 'agentModeChanged', active: false });
  
  // Remove border animation from ALL tabs
  chrome.tabs.query({}, (allTabs) => {
    allTabs.forEach((tab) => {
      try {
        chrome.tabs.sendMessage(tab.id, { action: 'agentEnd' }).catch(() => {
          // Ignore errors for tabs that can't receive messages
        });
      } catch (error) {
        // Ignore errors
      }
    });
  });
  
  announceToScreenReader('Agent mode disabled', 'polite');
}

// Note: Agent steps are now shown in the typing indicator shimmer

// Get page context for agent mode
async function getPageContextForAgent() {
  try {
    const tabs = await chrome.tabs.query({ active: true, currentWindow: true });
    const tab = tabs[0];
    
    if (!tab) return null;
    
    const results = await chrome.scripting.executeScript({
      target: { tabId: tab.id },
      func: () => {
        return {
          title: document.title,
          url: window.location.href,
          content: document.body.innerText.substring(0, 5000)
        };
      }
    });
    
    return results[0].result;
  } catch (error) {
    console.error('Error getting page context for agent:', error);
    return null;
  }
}

// Listen for agent step updates
chrome.runtime.onMessage.addListener((request, sender, sendResponse) => {
  if (request.action === 'agentStepUpdate') {
    
    // Show action indicator based on step status and title
    if (request.status === 'active') {
      const stepTitle = request.title || '';
      let action = 'working';
      let details = '';
      
      if (stepTitle.includes('planning_actions')) {
        action = 'planning';
        details = 'next steps';
      } else if (stepTitle.includes('write_content')) {
        action = 'writing';
        details = 'to document';
                    } else if (stepTitle.includes('search_google_maps')) {
                action = 'searching_maps';
        details = '';
      } else if (stepTitle.includes('scroll')) {
        action = 'scrolling';
        details = 'to target element';
      } else if (stepTitle.includes('click')) {
        action = 'clicking';
        details = 'on element';
      } else if (stepTitle.includes('extract_data')) {
        action = 'extracting';
        details = 'from page';
      } else if (stepTitle.includes('rewrite_text')) {
        action = 'rewriting';
        details = 'content';
      } else if (stepTitle.includes('summarize_content')) {
        action = 'summarizing';
        details = 'content';
      } else if (stepTitle.includes('fill_text')) {
        action = 'writing';
        details = 'in form field';
      }
      
      showAgentActionIndicator(action, details);
    } else if (request.status === 'completed') {
      // Show completion indicator briefly
      showAgentActionIndicator('completing', 'task finished');
      setTimeout(() => {
        hideTypingIndicator();
      }, 1000);
    }
  }
  
  if (request.action === 'audioTranscriptionResult') {
    handleAudioTranscriptionResult(request.transcript);
  }
});

// Context is always enabled - no toggle needed

// File handling
function handleFileSelection(e) {
  const files = Array.from(e.target.files);
  
  for (const file of files) {
    if (attachedFiles.length >= MAX_FILES) {
      announceToScreenReader(`Maximum ${MAX_FILES} files allowed`, 'assertive');
      break;
    }
    
    if (file.size > MAX_FILE_SIZE) {
      announceToScreenReader(`File ${file.name} exceeds 25MB limit`, 'assertive');
      continue;
    }
    
    attachedFiles.push(file);
    addAttachmentChip(file);
  }
  
  fileInput.value = ''; // Reset input
}

function addAttachmentChip(file) {
  if (!attachmentChips) {
    console.log('âŒ Attachment chips container not found');
    return;
  }
  
  attachmentChips.classList.remove('hidden');
  
  const chip = document.createElement('div');
  chip.className = 'attachment-chip';
  chip.dataset.fileName = file.name;
  
  const sizeKB = (file.size / 1024).toFixed(1);
  const fileIcon = getFileIcon(file.name);
  
  chip.innerHTML = `
    <div class="attachment-chip-icon">${fileIcon}</div>
    <span class="attachment-chip-name">${file.name}</span>
    <span class="attachment-chip-size">(${sizeKB}KB)</span>
    <button class="attachment-chip-remove" aria-label="Remove ${file.name}">
      ${icons.x}
    </button>
  `;
  
  chip.querySelector('.attachment-chip-remove').addEventListener('click', () => {
    removeAttachment(file.name);
    chip.remove();
    if (attachedFiles.length === 0) {
      attachmentChips.classList.add('hidden');
    }
  });
  
  attachmentChips.appendChild(chip);
}

function removeAttachment(fileName) {
  attachedFiles = attachedFiles.filter(f => f.name !== fileName);
}

// Read file content based on file type
async function readFileContent(file) {
  return new Promise((resolve, reject) => {
    const reader = new FileReader();
    
    reader.onload = (e) => {
      const content = e.target.result;
      
      // Handle different file types
      const extension = file.name.split('.').pop().toLowerCase();
      
      switch (extension) {
        case 'csv':
          resolve(content); // Return raw CSV
          break;
        case 'json':
          try {
            const parsed = JSON.parse(content);
            resolve(JSON.stringify(parsed, null, 2));
          } catch (err) {
            resolve(content); // Return raw if parsing fails
          }
          break;
        case 'txt':
        case 'md':
          resolve(content);
          break;
        case 'png':
        case 'jpg':
        case 'jpeg':
        case 'gif':
        case 'webp':
        case 'bmp':
        case 'svg':
          // For images, return base64 data URL for vision API
          resolve({
            type: 'image',
            name: file.name,
            mimeType: file.type,
            data: content, // This is the data URL (base64)
            size: file.size
          });
          break;
        default:
          // For other files, try to read as text
          resolve(content.substring(0, 50000)); // Limit to 50KB
      }
    };
    
    reader.onerror = () => {
      reject(new Error(`Failed to read file: ${file.name}`));
    };
    
    // Handle different file types
    const extension = file.name.split('.').pop().toLowerCase();
    if (['png', 'jpg', 'jpeg', 'gif', 'webp', 'bmp', 'svg'].includes(extension)) {
      reader.readAsDataURL(file);
    } else {
      reader.readAsText(file);
    }
  });
}

// Send message
async function sendMessage() {
  // Prevent double sends
  if (isSending) {
    console.log('Already sending, skipping duplicate...');
    return;
  }
  
  let message = chatInput.value.trim();
  
  // Validate message length - must be at least 1 character
  if (message.length < 1) {
    console.log('Message too short, not sending');
    
    // Add visual feedback for empty message
    const promptBox = document.getElementById('prompt-box');
    if (promptBox) {
      promptBox.style.animation = 'shake 0.5s ease-in-out';
      setTimeout(() => {
        promptBox.style.animation = '';
      }, 500);
    }
    
    // Show brief error message
    announceToScreenReader('Please enter a message before sending', 'assertive');
    return;
  }
  
  // Set flag after validation to prevent race conditions
  isSending = true;
  
  // Clear input
  chatInput.value = '';
  autoResizeTextarea();
  updateSendButtonState();
  
  // Add loading state to prompt box
  const promptBox = document.getElementById('prompt-box');
  promptBox?.classList.add('loading');
  
  // Remove empty state
  const emptyState = messagesContainer.querySelector('.empty-state');
  if (emptyState) emptyState.remove();
  
  // Add user message
  addMessage('user', message || '[Empty message]');
  
  // Add to history
  conversationHistory.push({ role: 'user', content: message });
  
  console.log('Message being sent:', message);
  console.log('Message length:', message.length);
  
  // Show typing indicator
  showTypingIndicator();
  
  // Process attached files
  let fileContext = '';
  if (attachedFiles.length > 0) {
    console.log('ğŸ“ Files attached:', attachedFiles.map(f => f.name));
    
    // Show file processing indicator
    showTypingIndicator('Processing files...', 'processing');
    
    for (const file of attachedFiles) {
      try {
        const fileData = await readFileContent(file);
        
        if (fileData.type === 'image') {
          // Handle image files
          fileContext += `\n\n[Image: ${fileData.name}]\nType: ${fileData.mimeType}\nSize: ${fileData.size} bytes\nData: ${fileData.data}`;
          console.log(`âœ… Processed image: ${fileData.name}`);
          
          // Show success message for image
          addMessage('system', `ğŸ–¼ï¸ **Image Processed**: ${fileData.name}\n\nImage has been added to your message context for analysis.`);
        } else {
          // Handle text files
          fileContext += `\n\n[File: ${fileData.name}]\n${fileData}`;
          console.log(`âœ… Processed file: ${fileData.name}`);
          
          // Show success message for text file
          addMessage('system', `ğŸ“ **File Processed**: ${fileData.name}\n\nFile content has been added to your message context.`);
        }
        
      } catch (error) {
        console.error(`Error reading file ${file.name}:`, error);
        addMessage('system', `âŒ **File Processing Failed**: ${file.name}\n\nCould not read file content: ${error.message}`);
      }
    }
    
    // Append file context to message
    message += fileContext;
    
    // Clear attachments after processing
    attachedFiles = [];
    if (attachmentChips) {
    attachmentChips.innerHTML = '';
    attachmentChips.classList.add('hidden');
    }
    
    // Hide file processing indicator
    hideTypingIndicator();
  }
  
  try {
    // Check which mode is currently active (only one can be active at a time)
    const isAgentMode = agentToggle?.classList.contains('active') || false;
    const isContextMode = true; // Always enabled
    
    console.log('Sending message with mode:', {
      isAgentMode,
      isContextMode,
      includeContext: true, // Always enabled
      message: message.substring(0, 50) + '...'
    });
    
    // Use Gemini API via background script with page context
    let response;
    if (isAgentMode) {
      // Show pause button for agent mode
      if (sendBtn) sendBtn.classList.add('hidden');
      if (pauseBtn) pauseBtn.classList.remove('hidden');
      
      // Show initial agent action
      showAgentActionIndicator('analyzing', 'page content');
      
      // Agent Mode: Execute actions
      response = await chrome.runtime.sendMessage({
        action: 'agentExecute',
        message: message,
        pageContext: await getPageContextForAgent()
      });
    } else {
      // Normal chat mode
      response = await chrome.runtime.sendMessage({
        action: 'geminiChat',
        message: message,
        history: conversationHistory,
        includeContext: true, // Always enabled
        agentMode: false,
        isContextMode: true // Always enabled
      });
    }
    
    console.log('ğŸ“¨ Response received:', response);
    
    if (response.success) {
        hideTypingIndicator();
      promptBox?.classList.remove('loading');
      
      // Show pause button during streaming
      if (sendBtn) sendBtn.classList.add('hidden');
      if (pauseBtn) pauseBtn.classList.remove('hidden');
      
      console.log('ğŸ“ Response content:', response.response);
      typewriterEffect(response.response);
      conversationHistory.push({ role: 'assistant', content: response.response });
      return;
        } else {
      // Show the helpful error message from the background script
      hideTypingIndicator();
      promptBox?.classList.remove('loading');
      console.log('âŒ Response error:', response.error);
      addMessage('assistant', response.response || response.error || 'Unknown error occurred');
      announceToScreenReader('Error occurred', 'assertive');
      return;
    }
    
  } catch (error) {
    console.error('Error sending message:', error);
    hideTypingIndicator();
    promptBox?.classList.remove('loading');
    
    let errorMessage = '';
    if (error.message.includes('Gemini API error')) {
      errorMessage = `Gemini API error: ${error.message}`;
    } else if (error.message.includes('fetch')) {
      errorMessage = 'Cannot connect to Gemini API. Please check your internet connection.';
    } else {
      errorMessage = `Error: ${error.message}. Please try again.`;
    }
    
    addMessage('assistant', errorMessage);
    announceToScreenReader('An error occurred', 'assertive');
  } finally {
    isSending = false; // Reset flag
  }
}

// Add message to display
function addMessage(role, content) {
  const messageDiv = document.createElement('div');
  messageDiv.className = `message ${role}`;
  
  // Add avatar for assistant messages
  if (role === 'assistant') {
    const avatarDiv = document.createElement('div');
    avatarDiv.className = 'message-avatar';
    avatarDiv.innerHTML = '<img src="assets/Cloudey ICON.svg" alt="Cloudey" class="avatar-icon">';
    messageDiv.appendChild(avatarDiv);
  }
  
  const contentDiv = document.createElement('div');
  contentDiv.className = 'message-content';
  contentDiv.textContent = content;
  
  messageDiv.appendChild(contentDiv);
  messagesContainer.appendChild(messageDiv);
  
  setTimeout(() => {
    messagesContainer.scrollTop = messagesContainer.scrollHeight;
  }, 100);
}

// Typewriter effect
async function typewriterEffect(text, speed = 30) {
  isStreaming = true;
  typewriterAbortController = new AbortController();
  
  // Show pause button
  if (sendBtn) sendBtn.classList.add('hidden');
  if (pauseBtn) pauseBtn.classList.remove('hidden');
  if (stopBtn) stopBtn.classList.add('hidden');
  
  const messageDiv = document.createElement('div');
  messageDiv.className = 'message assistant';
  
  // Add avatar for assistant messages
  const avatarDiv = document.createElement('div');
  avatarDiv.className = 'message-avatar';
  avatarDiv.innerHTML = '<img src="assets/Cloudey ICON.svg" alt="Cloudey" class="avatar-icon">';
  messageDiv.appendChild(avatarDiv);
  
  const contentDiv = document.createElement('div');
  contentDiv.className = 'message-content typing';
  
  messageDiv.appendChild(contentDiv);
  messagesContainer.appendChild(messageDiv);
  
  announceToScreenReader('Assistant is responding. Press S to skip animation.', 'polite');
  
  let currentText = '';
  
  try {
    for (let i = 0; i < text.length; i++) {
      if (typewriterAbortController.signal.aborted) {
        contentDiv.textContent = text;
        break;
      }
      
      currentText += text[i];
      contentDiv.textContent = currentText;
      
      messagesContainer.scrollTop = messagesContainer.scrollHeight;
      
      const adjustedSpeed = text.length > 500 ? speed * 0.6 : speed;
      await sleep(adjustedSpeed);
    }
  } catch (error) {
    contentDiv.textContent = text;
  }
  
  contentDiv.classList.remove('typing');
  
  isStreaming = false;
  typewriterAbortController = null;
  
  if (stopBtn) stopBtn.classList.add('hidden');
  if (pauseBtn) pauseBtn.classList.add('hidden');
  if (sendBtn) sendBtn.classList.remove('hidden');
  
  announceToScreenReader('Response complete', 'polite');
}

function skipTypewriter() {
  if (typewriterAbortController) {
    typewriterAbortController.abort();
  }
}

function stopGeneration() {
  if (typewriterAbortController) {
    typewriterAbortController.abort();
  }
  hideTypingIndicator();
  isStreaming = false;
  
  // Also stop agent mode if active
  if (agentToggle && agentToggle.classList.contains('active')) {
    deactivateAgentMode();
    addMessage('assistant', 'Agent mode stopped by user.');
  }
  
  if (stopBtn) stopBtn.classList.add('hidden');
  if (pauseBtn) pauseBtn.classList.add('hidden');
  if (sendBtn) sendBtn.classList.remove('hidden');
  announceToScreenReader('Generation stopped', 'polite');
}

function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

// Debounce helper for performance
function debounce(func, wait) {
  let timeout;
  return function executedFunction(...args) {
    const later = () => {
      clearTimeout(timeout);
      func(...args);
    };
    clearTimeout(timeout);
    timeout = setTimeout(later, wait);
  };
}

// Throttle helper for scroll performance
function throttle(func, limit) {
  let inThrottle;
  return function(...args) {
    if (!inThrottle) {
      func.apply(this, args);
      inThrottle = true;
      setTimeout(() => inThrottle = false, limit);
    }
  };
}

// Typing indicator
function showTypingIndicator() {
  assistantTypingRow.classList.remove('hidden');
  
  // Position the typing indicator at the top of messages
  if (messagesContainer) {
    messagesContainer.appendChild(assistantTypingRow);
  }
  
  messagesContainer.scrollTop = messagesContainer.scrollHeight;
  announceToScreenReader('Assistant is thinking', 'polite');
}

// Agent action indicator with shimmer effects
function showAgentActionIndicator(action, details = '') {
  const typingStatus = document.querySelector('.typing-status');
  const typingName = document.querySelector('.typing-name');
  
  if (typingStatus && typingName) {
    // Update the indicator content
    typingName.textContent = 'Cloudey Agent';
    
    // Set action-specific text and colors
    switch (action) {
      case 'analyzing':
        typingStatus.textContent = 'Analyzing page content...';
        typingStatus.style.color = 'rgba(100, 200, 255, 0.8)';
        break;
      case 'planning':
        typingStatus.textContent = 'Planning actions...';
        typingStatus.style.color = 'rgba(255, 200, 100, 0.8)';
        break;
      case 'writing':
        typingStatus.textContent = 'Writing content...';
        typingStatus.style.color = 'rgba(100, 255, 100, 0.8)';
        break;
      case 'scrolling':
        typingStatus.textContent = 'Scrolling to target...';
        typingStatus.style.color = 'rgba(255, 100, 255, 0.8)';
        break;
      case 'clicking':
        typingStatus.textContent = 'Clicking element...';
        typingStatus.style.color = 'rgba(255, 150, 100, 0.8)';
        break;
      case 'extracting':
        typingStatus.textContent = 'Extracting data...';
        typingStatus.style.color = 'rgba(150, 100, 255, 0.8)';
        break;
      case 'rewriting':
        typingStatus.textContent = 'Rewriting content...';
        typingStatus.style.color = 'rgba(100, 255, 200, 0.8)';
        break;
      case 'summarizing':
        typingStatus.textContent = 'Summarizing content...';
        typingStatus.style.color = 'rgba(255, 255, 100, 0.8)';
        break;
      case 'searching_maps':
        typingStatus.textContent = 'Searching Google Maps...';
        typingStatus.style.color = 'rgba(100, 200, 255, 0.8)';
        break;
      case 'completing':
        typingStatus.textContent = 'Completing task...';
        typingStatus.style.color = 'rgba(200, 200, 200, 0.8)';
        break;
      default:
        typingStatus.textContent = 'Working...';
        typingStatus.style.color = 'rgba(180, 160, 190, 0.8)';
    }
    
    // Add details if provided
    if (details) {
      typingStatus.textContent += ` (${details})`;
    }
    
    // Show the indicator
    assistantTypingRow.classList.remove('hidden');
    
    // Position at top of messages
    if (messagesContainer) {
      messagesContainer.appendChild(assistantTypingRow);
    }
    
    messagesContainer.scrollTop = messagesContainer.scrollHeight;
  }
}

function hideTypingIndicator() {
  assistantTypingRow.classList.add('hidden');
}

// Update send button state
function updateSendButtonState() {
  const hasContent = chatInput.value.trim().length > 0 || attachedFiles.length > 0;
  sendBtn.disabled = !hasContent;
}

// FAB actions
async function handleFabAction(action) {
  try {
    const [tab] = await chrome.tabs.query({ active: true, currentWindow: true });
    
    switch (action) {
      case 'summarize':
        addMessage('user', 'Summarize this page');
        showTypingIndicator();
        chrome.runtime.sendMessage(
          { action: 'summarizePage', tabId: tab.id },
          handleFabResponse
        );
        break;
        
      case 'improve':
        const selectedText = await getSelectedText(tab.id);
        if (selectedText) {
          addMessage('user', `Improve this: "${selectedText}"`);
          showTypingIndicator();
          chrome.runtime.sendMessage(
            { action: 'improveText', text: selectedText },
            handleFabResponse
          );
        } else {
          announceToScreenReader('Please select some text first', 'assertive');
        }
        break;
        
      case 'rewrite':
        const textToRewrite = await getSelectedText(tab.id);
        if (textToRewrite) {
          addMessage('user', `Rewrite this: "${textToRewrite}"`);
          showTypingIndicator();
          chrome.runtime.sendMessage(
            { action: 'rewriteText', text: textToRewrite },
            handleFabResponse
          );
        } else {
          announceToScreenReader('Please select some text first', 'assertive');
        }
        break;
        
      case 'translate':
        console.log('ğŸŒ Translation requested');
        const textToTranslate = await getSelectedText(tab.id);
        console.log('Selected text:', textToTranslate);
        
        if (textToTranslate && textToTranslate.trim().length > 0) {
          addMessage('user', `Translate this: "${textToTranslate}"`);
          showTypingIndicator();
          
          console.log('Sending translation request to background script...');
          chrome.runtime.sendMessage(
            { action: 'translate', text: textToTranslate, from: 'auto', to: 'en' },
            (response) => {
              console.log('Translation response received:', response);
              handleFabResponse(response);
            }
          );
        } else {
          console.log('No text selected for translation');
          announceToScreenReader('Please select some text first', 'assertive');
        }
        break;
    }
  } catch (error) {
    console.error('FAB action error:', error);
    hideTypingIndicator();
    addMessage('assistant', 'An error occurred. Please try again.');
  }
}

function handleFabResponse(response) {
  hideTypingIndicator();
  if (response && response.success) {
    // Handle translation responses specially
    if (response.result && response.from && response.to) {
      const translatedText = response.result;
      const detectedLanguage = response.detectedLanguage || response.from || 'auto';
      const targetLanguage = response.to || 'en';
      const method = response.method || 'unknown';
      
      // Show translation in chat with method indicator
      let methodIcon = 'ğŸŒ';
      if (method === 'chrome_translator_api') {
        methodIcon = 'âš¡'; // Chrome Translator API
      } else if (method === 'chrome_api') {
        methodIcon = 'ğŸ”§'; // Chrome i18n API
      }
      
      const translationMessage = `${methodIcon} **Translation** (${detectedLanguage} â†’ ${targetLanguage}):\n\n${translatedText}`;
      typewriterEffect(translationMessage);
    } else {
      // Handle other responses normally
    typewriterEffect(response.response);
    }
  } else {
    addMessage('assistant', response?.response || 'Feature coming soon!');
  }
}

async function getSelectedText(tabId) {
  try {
    console.log('Getting selected text from tab:', tabId);
    const results = await chrome.scripting.executeScript({
      target: { tabId: tabId },
      func: () => {
        const selection = window.getSelection();
        const selectedText = selection.toString();
        console.log('Selected text in page:', selectedText);
        return selectedText;
      }
    });
    const selectedText = results[0]?.result || '';
    console.log('Retrieved selected text:', selectedText);
    return selectedText;
  } catch (error) {
    console.error('Error getting selected text:', error);
    return '';
  }
}

// Screen reader announcements
function announceToScreenReader(message, priority = 'polite') {
  const element = priority === 'assertive' ? ariaAssertive : ariaPolite;
  element.textContent = message;
  setTimeout(() => {
    element.textContent = '';
  }, 1000);
}

// Load conversation history
window.addEventListener('load', () => {
  initializeIcons();
  updateSendButtonState();
  
  chrome.runtime.sendMessage(
    { action: 'getChatHistory' },
    (response) => {
      if (response && response.history && response.history.length > 0) {
        conversationHistory = response.history;
        const emptyState = messagesContainer.querySelector('.empty-state');
        if (emptyState) emptyState.remove();
        
        response.history.forEach(msg => {
          addMessage(msg.role, msg.content);
        });
      }
    }
  );
});

// Chrome AI flags check removed - using Gemini API for AI chat





// Initialize sidebar when DOM loads
document.addEventListener('DOMContentLoaded', () => {
  // Initialize translate button
  const translateBtn = document.getElementById('translate-btn');
  if (translateBtn) {
    translateBtn.addEventListener('click', async () => {
      console.log('ğŸŒ Translate button clicked');
      try {
        const tabs = await chrome.tabs.query({ active: true, currentWindow: true });
        const tab = tabs[0];
        
        if (tab) {
          const textToTranslate = await getSelectedText(tab.id);
          console.log('Selected text for translation:', textToTranslate);
          
          if (textToTranslate && textToTranslate.trim().length > 0) {
            addMessage('user', `Translate this: "${textToTranslate}"`);
            showTypingIndicator();
            
            console.log('Sending translation request to background script...');
            chrome.runtime.sendMessage(
              { action: 'translate', text: textToTranslate, from: 'auto', to: 'en' },
              (response) => {
                console.log('Translation response received:', response);
                handleFabResponse(response);
              }
            );
          } else {
            console.log('No text selected for translation');
            announceToScreenReader('Please select some text first', 'assertive');
          }
        }
      } catch (error) {
        console.error('Translate button error:', error);
        addMessage('assistant', 'An error occurred while translating. Please try again.');
      }
    });
  }
});

console.log('Cloudey side panel initialized');
